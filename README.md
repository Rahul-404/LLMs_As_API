# **LLM Multimodal API with FastAPI and Langserve**

This project creates a multi-Language Model (LLM) system that allows you to switch between different LLMs for different routes using **FastAPI**. The API is built to be extensible, allowing you to integrate and scale new LLMs as needed.

Additionally, the project includes a **Streamlit-based frontend** (in `client.py`) to provide a user-friendly interface for interacting with the API.

### **Key Features**
- **FastAPI Backend** to serve multiple LLMs through different routes.
- **Langserve Integration** to manage the interaction with multiple LLMs.
- **Streamlit Frontend** to easily interact with the backend via a simple web-based interface.
- **Demo Video** showcasing the working of the project and how to interact with the API.

---

## **Table of Contents**

- [Installation](#installation)
- [Project Structure](#project-structure)
- [How to Run the Project](#how-to-run-the-project)
- [Routes and API Endpoints](#routes-and-api-endpoints)
- [Streamlit Frontend](#streamlit-frontend)
- [Project Demo Video](#project-demo-video)
- [Contributing](#contributing)
- [License](#license)

---

## **Installation**

To get started with this project, clone the repository and install the dependencies:

```bash
git clone https://github.com/your-username/llm-multimodal-api.git
cd llm-multimodal-api
```

### Install dependencies:

```bash
# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install required Python packages
pip install -r requirements.txt
```

**Requirements**:
- **Python 3.8+**
- **FastAPI**
- **Langserve** (for handling LLMs)
- **Uvicorn** (for running the FastAPI server)
- **Streamlit** (for the frontend interface)
- **Other dependencies** listed in `requirements.txt`

---

## **Project Structure**

The project is organized as follows:

```
llm_api/
│
├── app.py              # FastAPI application serving routes
├── client.py           # Streamlit-based frontend to interact with the API
├── models/             # Models and configuration for Langserve
├── requirements.txt    # List of required Python packages
└── README.md           # Project documentation
```

- `**app.py**`: Contains FastAPI routes and logic for interacting with different LLMs using Langserve.
- `**client.py**`: The **Streamlit-based frontend** to interact with the API.
- `**models/**`: Contains configurations and models for Langserve.

---

## **How to Run the Project**

### 1. **Run the Backend (FastAPI)**

In the root project directory, you can start the FastAPI app with Uvicorn:

```bash
uvicorn app:app --reload
```

This will run the FastAPI server locally at `http://127.0.0.1:8000`.

### 2. **Run the Streamlit Frontend (client.py)**

Once the backend is running, you can start the Streamlit-based frontend by running the following command:

```bash
streamlit run client.py
```

This will open a Streamlit app in your browser, typically at `http://localhost:8501`.

### 3. **Access API Documentation**

FastAPI automatically generates interactive documentation. You can view it by navigating to the following URL:

```
http://127.0.0.1:8000/docs
```

Here you can test different routes and see how to interact with the API.

---

## **Routes and API Endpoints**

Below are the key routes available through the FastAPI app:

- **POST `/api/ask`**: Sends a query to the selected LLM.
  - **Parameters**:
    - `model` (str): The LLM model to use (e.g., "gpt-3", "llama").
    - `query` (str): The question or prompt to ask the model.
  - **Response**: The response generated by the LLM.

Example Request:

```bash
POST http://127.0.0.1:8000/api/ask
{
  "model": "gpt-3",
  "query": "What is the capital of France?"
}
```

---

## **Streamlit Frontend**

The `client.py` script uses **Streamlit** to create an easy-to-use frontend to interact with the API. The app allows you to:
- Select the LLM you want to use.
- Enter a query or prompt for the selected model.
- View the response returned by the API.

### How to Use:
1. Run the Streamlit app by executing:
   ```bash
   streamlit run client.py
   ```
2. Open the app in your browser (`http://localhost:8501` by default).
3. Choose a model (e.g., "gpt-3", "llama") from the dropdown menu.
4. Type your query into the text box and click **Submit**.
5. The model's response will be displayed below.

Streamlit makes it very easy to interact with your FastAPI backend through a simple, interactive web interface.

---

## **Project Demo Video**

To provide an overview of how this project works, we've included a **demo video**. This video demonstrates how to:
- Set up the FastAPI server.
- Interact with the backend using the **Streamlit frontend**.
- Switch between different LLMs and receive responses.

### **How to Watch the Demo Video**:
1. Open the video below to see a full demonstration of how the system works.
2. The video will explain:
   - Setting up the FastAPI server.
   - How to use the Streamlit-based frontend.
   - Sending requests to different LLMs.

   **You can view the video here:**

   - [Project Demo Video](path/to/your-demo-video.mp4) *(replace with your actual video link or local file path)*

> **Note**: Please ensure you have the necessary software installed (like a video player or browser) to watch the video.

---

## **Contributing**

We welcome contributions! If you'd like to contribute to this project, feel free to submit a pull request with your proposed changes. 

Please ensure you follow these steps before submitting:
- Fork the repository.
- Clone your forked repository to your local machine.
- Create a new branch for your feature or bugfix.
- Test your changes locally.
- Commit your changes and push them to your fork.
- Open a pull request with a detailed description of the changes.

---

## **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
